{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678b0ad9-72c2-4124-9ae2-b76299b67e04",
   "metadata": {},
   "source": [
    "# Fine-tuning Mistral Instruct\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Welcome to this comprehensive guide on fine-tuning Mistral Instruct for chatbot applications. In the ever-evolving landscape of artificial intelligence, chatbots have become an essential tool for businesses and developers alike, providing a seamless and efficient way to interact with users. Fine-tuning pre-trained language models like Mistral Instruct allows us to harness the power of state-of-the-art natural language processing (NLP) techniques to create sophisticated and responsive chatbots tailored to specific use cases.\n",
    "\n",
    "In this notebook, we will explore the step-by-step process of customizing Mistral Instruct to enhance its performance in conversational contexts. Whether you aim to develop a customer support assistant, an interactive FAQ bot, or a personal assistant, this guide will provide you with the necessary knowledge and techniques to achieve your goals.\n",
    "\n",
    "Key topics covered in this notebook include:\n",
    "\n",
    "1. **Understanding Mistral Instruct**: An overview of the Mistral Instruct model and its capabilities, highlighting why it is an excellent choice for chatbot development.\n",
    "2. **Data Preparation**: Techniques for curating and preprocessing datasets to ensure high-quality training data, crucial for effective model fine-tuning.\n",
    "3. **Fine-tuning Process**: Detailed instructions on configuring and running the fine-tuning process, including selecting appropriate hyperparameters and utilizing transfer learning strategies.\n",
    "4. **Evaluation and Optimization**: Methods to evaluate the performance of the fine-tuned model, along with optimization techniques to improve its accuracy and responsiveness.\n",
    "5. **Deployment Strategies**: Best practices for deploying your customized chatbot, ensuring it is scalable, reliable, and secure.\n",
    "\n",
    "By the end of this notebook, you will have a finely tuned Mistral Instruct model ready to be deployed as a chatbot, capable of understanding and responding to user queries with remarkable accuracy and relevance. Let's embark on this journey to create intelligent and engaging conversational agents, leveraging the power of Mistral Instruct to transform your chatbot applications.\n",
    "\n",
    "### Understanding Mistral Instruct\n",
    "\n",
    "Mistral Instruct is a powerful language model designed to facilitate a wide range of natural language processing tasks. Built on advanced neural network architectures, Mistral Instruct excels in understanding and generating human-like text, making it an ideal choice for developing interactive chatbots. Here, we delve into the key features and capabilities of Mistral Instruct, elucidating why it stands out for chatbot fine-tuning.\n",
    "\n",
    "**1. Pre-trained Knowledge Base**: Mistral Instruct comes with a robust pre-trained knowledge base, which has been trained on vast amounts of text data from diverse sources. This extensive training endows the model with a rich understanding of language nuances, context, and various domains, providing a solid foundation for further customization.\n",
    "\n",
    "**2. Instruction Following**: Unlike traditional language models, Mistral Instruct is specifically designed to follow instructions effectively. This capability is crucial for chatbot applications, where the model needs to respond accurately to user commands and inquiries. The instruction-following nature of Mistral Instruct ensures that the chatbot can adhere to specific guidelines and provide relevant information consistently.\n",
    "\n",
    "**3. Transfer Learning**: Mistral Instruct leverages transfer learning, allowing developers to fine-tune the model on smaller, task-specific datasets while retaining the general language understanding acquired during the pre-training phase. This approach significantly reduces the time and computational resources required for training, enabling efficient customization for chatbot development.\n",
    "\n",
    "**4. Versatility**: The versatility of Mistral Instruct makes it suitable for a wide array of chatbot functionalities. Whether you need a model to handle customer service queries, provide product recommendations, or engage users in casual conversation, Mistral Instruct can be fine-tuned to meet the specific requirements of your application.\n",
    "\n",
    "**5. Contextual Understanding**: One of the standout features of Mistral Instruct is its ability to maintain context over extended interactions. This contextual understanding is vital for chatbots, as it allows them to generate coherent and contextually relevant responses, enhancing the user experience.\n",
    "\n",
    "**6. Scalability**: Mistral Instruct is designed to scale efficiently, making it suitable for deployment in environments with varying loads. Whether your chatbot needs to handle a few interactions or thousands simultaneously, Mistral Instruct can be scaled to meet the demand without compromising performance.\n",
    "\n",
    "By understanding these core features and capabilities of Mistral Instruct, you can appreciate why it is an excellent choice for fine-tuning and deploying chatbots. In the following sections, we will guide you through the process of preparing data, fine-tuning the model, and optimizing its performance to create a highly effective and engaging chatbot.\n",
    "\n",
    "Let's start by importing relevant libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390d9a8-360a-4004-aecd-84d9be5cd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, PeftConfig\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from typing import Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from awq import AutoAWQForCausalLM\n",
    "import json\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d98df-3dcd-4f8b-af04-abbd5832df7b",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Effective data preparation is a crucial step in fine-tuning Mistral Instruct for your chatbot application. The quality and relevance of your training data directly impact the performance and accuracy of the model. In this section, we will focus on curating and preprocessing datasets specifically for La Roche-Posay, a well-known skincare brand, ensuring that the chatbot can provide accurate and helpful responses related to skincare products and routines.\n",
    "\n",
    "#### 1. Collecting Data\n",
    "\n",
    "The first step is to collect a diverse set of conversational data related to the task. This includes:\n",
    "\n",
    "- **Customer Support Conversations**: Extract dialogues from customer support channels where common skincare concerns and product inquiries are discussed.\n",
    "- **Product Descriptions**: Gather detailed descriptions of skin care products, including ingredients, usage instructions, and benefits.\n",
    "- **FAQs**: Compile a list of frequently asked questions and answers from the company website or customer support.\n",
    "- **Reviews and Feedback**: Analyze customer reviews and feedback to understand common sentiments and issues.\n",
    "\n",
    "#### 2. Structuring Data\n",
    "\n",
    "To ensure consistency and facilitate training, the collected data should be structured in a standardized format. We will use a simple JSON format where each conversation entry includes the following fields:\n",
    "\n",
    "- **Context**: The initial query or context provided by the user.\n",
    "- **Response**: The expected response from the chatbot.\n",
    "\n",
    "Example:\n",
    "```json\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\n",
    "      \"prompt\": \"Can you tell me more about the pimple serum?\",\n",
    "      \"completion\": \"Our serum is a dual-action acne treatment formulated with benzoyl peroxide and lipo-hydroxy acid. It helps to reduce acne, prevent future breakouts, and minimize the appearance of pores.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Another solution, which is applied here and highly suggested is that you format the messages yourself. When finetuning Mistral models, the expected for proper training is:\n",
    "```python\n",
    "\"<s>[INST] _context_ [/INST] _expected_chatbot_response </s>\"\n",
    "```\n",
    "This format will insure consistency and reduce any possibility of the model misunderstanding the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3e0b9-5069-452f-94f6-5a685b442791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset =load_dataset('json', data_files='../data/lrp/finetune_dataset.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939f486-a078-4348-822a-a64913bbd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_data = [conv[1]['content'] for conv in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad871e13-4cdb-4198-9d2f-bbde2d027c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_format(example):\n",
    "    system_prompt = \"\"\"\n",
    "          [Company_name] is a skin-care laboratory, delivering dermatologic products throughout the world.\n",
    "          You are their community manager.\n",
    "          \"\"\"\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    prompt= f\"<s>{B_INST}{system_prompt}\\n{example['prompt']}\\n{E_INST}{example['completion']}</s>\"\n",
    "    example['chat'] = prompt\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99545bc-39ff-49bc-9e91-9a34529b9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(chat_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050f33c-8110-49db-8df3-9b177c961140",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  dataset.remove_columns(['prompt','completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa50890-d8c1-42c1-b8c5-bf0f4767f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(example):\n",
    "    example = tokenizer(\n",
    "        example['chat'],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d0c82-626c-47cc-808f-06e211ddd920",
   "metadata": {},
   "source": [
    "### Summary of the Fine-Tuning Process\n",
    "\n",
    "The fine-tuning process for the Mistral Instruct model involves several key steps and parameter choices to optimize its performance as a chatbot. Here is an overview of the key components and their configurations:\n",
    "\n",
    "#### Model and Output\n",
    "- **Model ID**: `\"mistralai/Mistral-7B-Instruct-v0.1\"`\n",
    "- **New Model Name**: `\"../output/Mistral-qlora-7B-Instruct-v0.1-5\"`\n",
    "\n",
    "#### QLoRA Parameters\n",
    "- **LoRA Attention Dimension (`lora_r`)**: Set to 64, this parameter controls the dimension of the low-rank adaptation matrices.\n",
    "- **LoRA Alpha (`lora_alpha`)**: Set to 16, it scales the updates from the LoRA layers.\n",
    "- **LoRA Dropout (`lora_dropout`)**: Set to 0.1, it specifies the dropout probability for LoRA layers to prevent overfitting.\n",
    "\n",
    "#### BitsAndBytes Parameters\n",
    "- **4-bit Precision (`use_4bit`)**: Enabled for efficient model loading.\n",
    "- **Compute Dtype for 4-bit Models (`bnb_4bit_compute_dtype`)**: Set to `\"bfloat16\"` to balance precision and memory usage.\n",
    "- **Quantization Type (`bnb_4bit_quant_type`)**: Set to `\"nf4\"` for efficient computation.\n",
    "- **Nested Quantization (`use_nested_quant`)**: Enabled to allow double quantization, further reducing memory footprint.\n",
    "\n",
    "We are working with a quantized model (using 4-bit precision) and fine-tuning with LoRA (Low-Rank Adaptation). This combination is advantageous because it significantly reduces the memory and computational resources required for training, making it feasible to fine-tune large models like Mistral Instruct on consumer-grade hardware without sacrificing much performance.\n",
    "\n",
    "#### TrainingArguments Parameters\n",
    "- **Output Directory (`output_dir`)**: `\"./results\"`, where model predictions and checkpoints are stored.\n",
    "- **Number of Training Epochs (`num_train_epochs`)**: Set to 1, indicating a single pass over the training dataset.\n",
    "- **Mixed Precision Training (`fp16` and `bf16`)**: Both disabled in this setup.\n",
    "- **Batch Sizes (`per_device_train_batch_size` and `per_device_eval_batch_size`)**: Both set to 4, determining the number of samples per GPU during training and evaluation.\n",
    "- **Gradient Accumulation Steps (`gradient_accumulation_steps`)**: Set to 1, controlling how often gradients are updated.\n",
    "- **Gradient Checkpointing**: Enabled to save memory during training.\n",
    "- **Maximum Gradient Norm (`max_grad_norm`)**: Set to 0.3, for gradient clipping to stabilize training.\n",
    "- **Learning Rate (`learning_rate`)**: Set to 0.0002, defining the step size for the optimizer.\n",
    "- **Weight Decay (`weight_decay`)**: Set to 0.001 to regularize model weights.\n",
    "- **Optimizer (`optim`)**: `\"paged_adamw_32bit\"`, chosen for efficient optimization.\n",
    "- **Learning Rate Schedule (`lr_scheduler_type`)**: Set to `\"constant\"` for a stable learning rate throughout training.\n",
    "- **Max Training Steps (`max_steps`)**: Set to -1, indicating that `num_train_epochs` is used.\n",
    "- **Warmup Ratio (`warmup_ratio`)**: Set to 0.03 for gradual learning rate increase at the start of training.\n",
    "- **Sequence Grouping (`group_by_length`)**: Enabled to improve training efficiency by batching sequences of similar lengths.\n",
    "- **Checkpoint Saving (`save_steps`)**: Save model checkpoints every 25 steps.\n",
    "- **Logging (`logging_steps`)**: Log training progress every 25 steps.\n",
    "\n",
    "#### SFT Parameters\n",
    "- **Maximum Sequence Length (`max_seq_length`)**: Set to 1024, defining the maximum length of input sequences.\n",
    "- **Sequence Packing (`packing`)**: Disabled, indicating single-sequence inputs.\n",
    "- **Device Map (`device_map`)**: Configured to use GPU 0 for model loading.\n",
    "\n",
    "#### Model and Tokenizer Configuration\n",
    "- **Model Loading**: The model is loaded with the QLoRA configuration, incorporating the specified `bnb_config`.\n",
    "- **Tokenization**: The MistralAi tokenizer is loaded and adjusted to include a special padding token `<pad>` and set to pad on the right.\n",
    "- **Embedding Resize**: The model's token embeddings are resized to match the tokenizer's vocabulary size.\n",
    "\n",
    "#### PEFT Configuration\n",
    "- **LoRA Configuration (`peft_config`)**: Includes parameters like `lora_alpha`, `lora_dropout`, `r`, and target modules (e.g., `q_proj`, `k_proj`, `v_proj`) for low-rank adaptation.\n",
    "\n",
    "This configuration is designed to optimize the Mistral Instruct model for effective and efficient fine-tuning, enabling it to perform well as a chatbot. By leveraging quantization and LoRA, we ensure that the model is not only powerful but also resource-efficient, making it accessible for a wider range of applications and hardware configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f81575-9131-4545-9017-a53b239b5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "new_model = \"../output/Mistral-qlora-7B-Instruct-v0.1-5\" #set the name of the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8eaaaf-a8b5-403d-87e5-867f954f8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = True\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "#device_map = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6189b-2a5d-4b96-8ff8-07be16feebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479f0b7-ebfb-4e55-8c06-d95419128c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e08ed-7e11-4bdc-9f7f-f2d1f211a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MitsralAi tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "tokenizer.padding_side='right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b2f46-f9f5-4148-b249-b9d85b84caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a71147-20a4-4ea0-8bdd-c9b89daa2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\"\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=25,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=100, # the total number of training steps to perform\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96217fbc-6cb6-45db-a61f-74daa53b1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fe1bb-8b3b-412d-be8a-85ca79180e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer for fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=256,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    "    dataset_text_field='chat'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a82f11-7b6e-42d2-a081-7b31011509e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e524a63-f00e-4ce1-809b-ff765cc9ed0c",
   "metadata": {},
   "source": [
    "### Properly saving the model\n",
    "\n",
    "Last step in the fine-tuning the model is properly saving it. When fine-tuning with LoRA, the  *save_pretrained* function will only save LoRA adaptors, and not the fine-tuned model. In order to have the model ready for future use, you will have to follow these steps:\n",
    "1. Save the LoRA adaptors after fine-tuning\n",
    "2. Reload the base model\n",
    "3. Merge the model with LoRA adaptors\n",
    "4. Save the merged model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6edbe9-884b-4d30-885a-6931f8b4d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4aa6b-2d15-41da-bb72-f6b24f0e43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    #device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae64981-0eb1-4fc5-b576-d88d0079cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model= PeftModel.from_pretrained(base_model, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0f622-1998-4b19-823b-39c883d1dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model= merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f6898-a94b-487b-9e2d-703a6184f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_save_dir = \"../output/Mistral-Finetuned-Merged-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9ce2f-86a8-4027-bc89-1be8b939498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(sagemaker_save_dir, safe_serialization=True)\n",
    "# save tokenizer for easy inference\n",
    "tokenizer.save_pretrained(sagemaker_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09914e6-3f9a-43fe-bd09-43a7cc402f3a",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The evaluation of our fine-tuned Mistral Instruct model will be conducted in two parts: human evaluation and automated metrics. This comprehensive approach ensures both qualitative and quantitative insights into the model's performance.\n",
    "\n",
    "**1. Human Evaluation**\n",
    "\n",
    "In the first part of our evaluation, we will conduct human evaluations by engaging a streamer to interact with the chatbot. This will involve asking a variety of questions to assess the chatbot's ability to understand and respond accurately and naturally. Human evaluation is crucial as it provides real-world feedback on the conversational quality, relevance, and user satisfaction, which are difficult to measure through automated metrics alone.\n",
    "\n",
    "**2. Automated Metrics**\n",
    "\n",
    "The second part involves using established automated metrics to quantitatively measure the performance of our model. The metrics we will use include:\n",
    "\n",
    "- **F1 Score**: This metric will be used to evaluate the accuracy of the model in terms of precision and recall. It is particularly useful in determining how well the chatbot can provide correct answers (precision) and how often it provides correct answers out of the total number of answers it should provide (recall).\n",
    "\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: ROUGE metrics (such as ROUGE-N, ROUGE-L) will help us evaluate the quality of the generated text by comparing it to reference texts. It is commonly used to assess the quality of summaries and generated responses in NLP tasks.\n",
    "\n",
    "- **BLEU (Bilingual Evaluation Understudy)**: BLEU score will be used to evaluate the accuracy and fluency of the chatbot's responses by comparing them with reference responses. It is particularly useful for tasks involving machine translation and text generation.\n",
    "\n",
    "- **Perplexity**: This metric will be used to measure how well the probability distribution predicted by the model aligns with the actual distribution of the test data. Lower perplexity indicates better predictive performance, reflecting the model's ability to generate coherent and contextually appropriate responses.\n",
    "\n",
    "By combining human evaluations with these automated metrics, we aim to obtain a holistic understanding of the chatbot's performance, ensuring it meets both qualitative and quantitative standards for effective and engaging conversations.\n",
    "\n",
    "For this part you might want to reload the model and the  tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c57aa-8cdc-4ff2-9254-9809c2974763",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "merged_model = merged_model.to('cuda')\n",
    "\n",
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = (\n",
    "        \"La Roche-Posay is a skin-care laboratory, delivering dermatologic products throughout the world.\\n\"\n",
    "        \"You are a community manager from La Roche-Posay.\"\n",
    "        \"Here are the rules you will follow while generating the answer:\\n\"\n",
    "        \"1. You do not have a name!\\n\"\n",
    "        \"2. The brand is not looking for collaborations at the moment!\\n\"\n",
    "        \"3. The brand does not send out free samples at the moment!\\n\"\n",
    "        \"4. Keep the answers precise and concrete!\\n\"\n",
    "        \"5. After the blue heart, stop generating new text!\\n\"\n",
    "        \"6. Never generate P.S. messages!\\n\"\n",
    "        \"7. All of the answers will end in \'Best regards\'!\n\"\n",
    "        \"8. The only allowed emoji is the blue heart ðŸ’™.\\n\"\n",
    "        \"9. You're not specific to any country!\\n\"\n",
    "        \"10. Never include hyperlinks!\\n\"\n",
    "        \"Never break these rules!\\n\"\n",
    "        \"Answer this user's question:\\n\"\n",
    "    )\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "    prompt = f\"{B_INST}{system_prompt}{user_prompt.strip()}\\n{E_INST}\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    _ = merged_model.generate(\n",
    "        **inputs\n",
    "        max_new_tokens=200, \n",
    "        streamer=streamer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67bda1-1c30-4efa-995f-87baf0922aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, I want to collab with all LRP, who do I contact?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1228b6-45bd-4315-bb15-fb0507a7d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, I have really bad acne, how do i deal with it?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4cd4f-3616-4db3-a095-2330ec6471e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, what do you reccommend for dark circles?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa97a1-7022-427c-b551-a8f08e2a8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, I have skin cancer, how do i deal with it?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a41687-9f78-4731-8a42-733b64a1c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, could you suggest a sunscreen?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cd529-49e7-4579-8ff0-c2f248994635",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, I want some free samples, can you send them?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4a846-3867-447d-9628-5487652784bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, I have always wanted to collaborate with your brand, how possible is it?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f6604-8683-4a6b-b7f5-b69d4c9aa0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi do you have a makeup line?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a6ae4-837f-4ce8-9ba6-cd2b77a08fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi how do i get rid of acne?\"\n",
    "stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd54a6-87f7-41f1-a10d-83cc820a4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, tokenizer_name_or_path=None, batch_size=8):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    tokenizer.paddidng_side='left'\n",
    "    \n",
    "    def calculate_perplexity(predictions, references):\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        input_ids = tokenizer(references, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        return torch.exp(loss).item()\n",
    "\n",
    "    def calculate_bleu(predictions, references):\n",
    "        return np.mean([sentence_bleu([ref.split()], pred.split()) for pred, ref in zip(predictions, references)])\n",
    "\n",
    "    def calculate_rouge(predictions, references):\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = [scorer.score(ref, pred) for pred, ref in zip(predictions, references)]\n",
    "        avg_scores = {\n",
    "            'rouge1': np.mean([score['rouge1'].fmeasure for score in scores]),\n",
    "            'rouge2': np.mean([score['rouge2'].fmeasure for score in scores]),\n",
    "            'rougeL': np.mean([score['rougeL'].fmeasure for score in scores])\n",
    "        }\n",
    "        return avg_scores\n",
    "\n",
    "    def calculate_f1(predictions, references):\n",
    "        pred_tokens = [tokenizer(pred, return_tensors=\"pt\").input_ids.squeeze().tolist() for pred in predictions]\n",
    "        ref_tokens = [tokenizer(ref, return_tensors=\"pt\").input_ids.squeeze().tolist() for ref in references]\n",
    "        \n",
    "        # Ensure the lengths match by padding/truncating\n",
    "        max_len = max(max(len(pt), len(rt)) for pt, rt in zip(pred_tokens, ref_tokens))\n",
    "        pred_tokens = [pt + [tokenizer.pad_token_id] * (max_len - len(pt)) for pt in pred_tokens]\n",
    "        ref_tokens = [rt + [tokenizer.pad_token_id] * (max_len - len(rt)) for rt in ref_tokens]\n",
    "        pred_flat = [item for sublist in pred_tokens for item in sublist]\n",
    "        ref_flat = [item for sublist in ref_tokens for item in sublist]\n",
    "        \n",
    "        # Convert token ids to binary format\n",
    "        pred_flat_binary = [1 if item != tokenizer.pad_token_id else 0 for item in pred_flat]\n",
    "        ref_flat_binary = [1 if item != tokenizer.pad_token_id else 0 for item in ref_flat]\n",
    "        \n",
    "        return f1_score(ref_flat_binary, pred_flat_binary, average='binary')\n",
    "\n",
    "    predictions, references = [], []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
    "        input_texts = batch['chat']\n",
    "        inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200\n",
    "            )\n",
    "        predictions.extend([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in outputs])\n",
    "        references.extend(input_texts)\n",
    "\n",
    "    perplexity = calculate_perplexity(predictions, references)\n",
    "    bleu = calculate_bleu(predictions, references)\n",
    "    rouge = calculate_rouge(predictions, references)\n",
    "    f1 = calculate_f1(predictions, references)\n",
    "\n",
    "    return {\n",
    "        'perplexity': perplexity,\n",
    "        'bleu': bleu,\n",
    "        'rouge': rouge,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033307ba-5067-4e2e-95c2-9f1dfb55bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(base_model, tokenizer, dataset.select(range(1)))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b8004-8ddf-444b-aaa7-e499142ec2c3",
   "metadata": {},
   "source": [
    "### Deployment Strategies\n",
    "\n",
    "In this section, we focus on deploying the fine-tuned Mistral Instruct model using vLLM, a highly efficient inference framework designed to maximize the performance and scalability of language models. To ensure our chatbot runs smoothly and efficiently, we utilized AWQ quantization, a technique that reduces the model size and speeds up inference without significant loss of accuracy.\n",
    "\n",
    "Key steps in our deployment process include:\n",
    "\n",
    "1. **Model Quantization**: We applied AWQ quantization to compress the model, optimizing it for faster inference while maintaining high performance. This step is crucial for deploying the chatbot in resource-constrained environments or at scale.\n",
    "2. **Setting Up vLLM**: Detailed instructions on configuring and deploying the vLLM framework, ensuring that our model leverages its advanced features for optimal efficiency.\n",
    "3. **Integration and Testing**: Steps to integrate the quantized model with vLLM and rigorous testing to verify the chatbot's responsiveness and accuracy in real-world scenarios.\n",
    "4. **Scalability and Maintenance**: Best practices for scaling the deployment to handle increased traffic and maintaining the system for long-term reliability and performance.\n",
    "\n",
    "By following these deployment strategies, you will be able to deploy your fine-tuned Mistral Instruct model as a robust and efficient chatbot, ready to provide high-quality interactions at scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e0328-45f1-4c0d-98eb-c0d6ec875c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"../output/Mistral-Finetuned-Merged-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac802b-4ce6-485a-925c-f29e7364f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c34ae8-c535-4c4e-9489-8b84f027dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_id, device_map=\"cpu\", safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74978c4-707a-430f-8379-708b35e7382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/lrp/finetune_dataset_output.jsonl'\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(output_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "calib_data = [conv['chat'] for conv in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77dac3-4781-43dc-91d9-31f7d719d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantize(tokenizer, quant_config=quant_config, calib_data=calib_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b6fc5-3ef3-421e-b3a0-aab87f7eb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "awq_model_id = \"../output/Mistral-Finetuned-Merged-5-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152ea39-9ce8-49e7-9411-5c896f4f566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(awq_model_id, safetensors=True, shard_size=\"4GB\")\n",
    "tokenizer.save_pretrained(awq_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11fe8ad-9982-47c8-826c-1210f85cc9b1",
   "metadata": {},
   "source": [
    "### Optional\n",
    "\n",
    "In some cases, having two *safetensors* files can create complications with inference, as some frameworks like vLLM only support one. Our workaround was simply combining the files into one, and modifying the *model.safeetensors.index.json* file to match the new *safetensors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2edf5-e702-46a1-a64d-2d47012207fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first safetensors file\n",
    "safetensor_path = \"../output/Mistral-Finetuned-Merged-5-AWQ/model-00001-of-00002.safetensors\"\n",
    "\n",
    "with safe_open(safetensor_path , framework=\"pt\") as f:\n",
    "    data1 = {}\n",
    "    for key in f.keys():\n",
    "       data1[key] = f.get_tensor(key)\n",
    "\n",
    "# Load the second safetensors file\n",
    "safetensor_path = \"../output/Mistral-Finetuned-Merged-5-AWQ/model-00002-of-00002.safetensors\"\n",
    "\n",
    "with safe_open(safetensor_path, framework=\"pt\") as f:\n",
    "    data2 = {}\n",
    "    for key in f.keys():\n",
    "       data2[key] = f.get_tensor(key)\n",
    "    \n",
    "\n",
    "# Merge the contents of both files\n",
    "merged_data = {**data1, **data2}\n",
    "\n",
    "output_file =  \"../output/merged_file_2.safetensors\"\n",
    "# Save the merged contents into a new safetensors file\n",
    "save_file(merged_data, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
